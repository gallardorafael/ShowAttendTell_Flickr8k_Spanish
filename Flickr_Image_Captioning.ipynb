{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Flickr_Image_Captioning.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyMT9v5QRyVEuwR61x1ltco7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Nzwdgjf-Vli6","colab_type":"text"},"source":["# Preparando el entorno: \n","* Importación de librerías\n","* Abriendo Google Drive"]},{"cell_type":"code","metadata":{"id":"w6OihrnBHjHe","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x \n","\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import re\n","import numpy as np\n","import json\n","from glob import glob\n","from PIL import Image\n","import pickle\n","import keras\n","import sys, time, os, warnings \n","import numpy as np\n","import pandas as pd \n","from collections import Counter \n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pu43vSMUVxTj","colab_type":"text"},"source":["Estableciendo las rutas para acceder a las imágenes y a las descripciones."]},{"cell_type":"code","metadata":{"id":"nJSCemQ5SrPM","colab_type":"code","colab":{}},"source":["# La ruta al directorio con las imagenes del dataset\n","dir_Flickr_jpg = '/content/drive/My Drive/Colab Notebooks/ImageCaptioning/Flickr8k/Flickr8k_Dataset/'\n","\n","# La ruta al archivo con las descripciones\n","dir_Flickr_text = '/content/drive/My Drive/Colab Notebooks/ImageCaptioning/Flickr8k/Flickr8k_texto/es_Flickr8k.token.txt'\n","\n","jpgs = os.listdir(dir_Flickr_jpg)\n","print(\"Cantidad de imagenes en Flicker8k: {}\".format(len(jpgs)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UJXS_rOWV2aj","colab_type":"text"},"source":["# Análisis preliminar de los datos:\n","* Guardando las descripciones y los IDs en un pandas dataframe"]},{"cell_type":"code","metadata":{"id":"4Fm5TejXWG48","colab_type":"code","colab":{}},"source":["# Leyendo las descripciones\n","file = open(dir_Flickr_text, 'r')\n","text = file.read()\n","file.close()\n","\n","# Extrayendo la información y guardando en el dataframe\n","lineas_txt = list()\n","for line in text.split('\\n'): # Separamos por líneas\n","  column = line.split(',') # Separamos por columnas\n","  if len(column) == 1:\n","    continue\n","  w = [column[0], column[1], column[2]]\n","  lineas_txt.append(w)\n","\n","captions_df = pd.DataFrame(lineas_txt,columns=['filename','caption_index','caption_text'])\n","print(captions_df.head())\n","\n","# Obteniendo la información de las descripciones\n","uni_filenames = np.unique(captions_df.filename.values)\n","print(\"Total de imgaénes: : {}\".format(len(uni_filenames)))\n","print(\"Cantidad de descripciones por imagen:\")\n","Counter(Counter(captions_df.filename.values).values())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gZEZVv5BcQWC","colab_type":"text"},"source":["Se observarán las descripciones a la par de la imagen que describen según el dataset."]},{"cell_type":"code","metadata":{"id":"WI7AAmXtdDo8","colab_type":"code","colab":{}},"source":["from keras.preprocessing.image import load_img, img_to_array\n","\n","npic = 5\n","npix = 224\n","target_size = (npix,npix,3)\n","\n","count = 1\n","fig = plt.figure(figsize=(10,20))\n","for jpgfnm in uni_filenames[:npic]:\n","    filename = dir_Flickr_jpg + '/' + jpgfnm\n","    captions = list(captions_df[\"caption_text\"].loc[captions_df[\"filename\"]==jpgfnm].values)\n","    image_load = load_img(filename, target_size=target_size)\n","    \n","    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n","    ax.imshow(image_load)\n","    count += 1\n","    \n","    ax = fig.add_subplot(npic,2,count)\n","    plt.axis('off')\n","    ax.plot()\n","    ax.set_xlim(0,1)\n","    ax.set_ylim(0,len(captions))\n","    for i, caption in enumerate(captions):\n","        ax.text(0,i,caption,fontsize=20)\n","    count += 1\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OnkYoZpc6JK4","colab_type":"text"},"source":["# Análisis de las descripciones\n"]},{"cell_type":"markdown","metadata":{"id":"Zg1u0lrr6elP","colab_type":"text"},"source":["Se creará un dataframe para visualizar la distribución de las palabras en nuestro conjunto de datos traducido, este nuevo dataframe contendrá cada palabra y su respectiva frecuencia en orden descendente."]},{"cell_type":"code","metadata":{"id":"IEmcS4R76qew","colab_type":"code","colab":{}},"source":["def df_word(df_txt):\n","  vocabulary = []\n","  for txt in df_txt.caption_text.values:\n","    vocabulary.extend(txt.split())\n","  print('Tamaño del vocabulario: %d' % len(set(vocabulary)))\n","  ct = Counter(vocabulary)\n","  dfword = pd.DataFrame.from_dict(ct, orient='index').sort_values(by=0, ascending=False)\n","  dfword = dfword.reset_index()\n","  dfword = dfword.rename({'index':'word',0:'count'}, axis=1)\n","  return(dfword)\n","\n","dfword = df_word(captions_df)\n","dfword.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wSNDLOwrA7tU","colab_type":"text"},"source":["Se visualizan las palabras más y menos frecuentes que aparecen en todo el vocabulario.\n","\n","Nota: Se puede observar que la mayoría de palabras en el vocabulario son articulos que no aportan mucha información sobre los datos."]},{"cell_type":"code","metadata":{"id":"JZyIc8X5Bv2u","colab_type":"code","colab":{}},"source":["top = 50\n","\n","def plthist(dfsub, title):\n","    plt.figure(figsize=(20,3))\n","    plt.bar(dfsub.index,dfsub[\"count\"])\n","    plt.yticks(fontsize=20)\n","    plt.xticks(dfsub.index,dfsub[\"word\"],rotation=90,fontsize=20)\n","    plt.title(title,fontsize=20)\n","    plt.show()\n","\n","plthist(dfword.iloc[:top,:], title=\"Las 50 palabras más frecuentes\")\n","plthist(dfword.iloc[-top:,:], title=\"Las 50 palabras menos frecuentes\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"voYPsAc3T0rY","colab_type":"text"},"source":["# Preparación de los datos\n","\n","* Se preparan los datos de forma separada a las imágenes\n","* Se desarrollará el modelo con \"Visual Attention\"\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"H6HHyqlPW_BD","colab_type":"code","colab":{}},"source":["from sklearn.utils import shuffle\n","\n","# Obteniendo todas las rutas a las imágenes\n","all_image_names = captions_df['filename'].tolist()\n","\n","all_captions_n = captions_df['caption_text'].tolist()\n","\n","all_img_name_vector = list()\n","for name in all_image_names:\n","  full_path = dir_Flickr_jpg + name\n","  all_img_name_vector.append(full_path)\n","\n","all_captions = list()\n","for caption in all_captions_n:\n","  new_caption = '<start> ' + caption + ' <end>'\n","  all_captions.append(new_caption)\n","\n","# Se hace un random para obtener el conjunto de entrenamiento\n","train_captions_, img_name_vector_ = shuffle(all_captions,\n","                                          all_img_name_vector,\n","                                          random_state=1)\n","\n","\n","\n","# Se utilizarán 8000 (de 8092) ejemplos para entrenamiento\n","num_examples = 35900 # Cantidad de descripciones\n","# Set de entrenamiento\n","train_captions = train_captions_[:num_examples]\n","img_name_vector = img_name_vector_[:num_examples]\n","\n","print(\"img_name_vector\", img_name_vector[:10])\n","print(\"train_captions\", train_captions[:10])\n","\n","# Set de prueba\n","test_captions = train_captions_[num_examples:]\n","fnm_test = img_name_vector_[num_examples:]\n","\n","print(\"Entrenamiento con: \",len(train_captions), \"descripciones, de un total de:\", len(all_captions))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"458jmWntiFht","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_8Cd7cZDhedU","colab_type":"text"},"source":["# Preprocesando las imágenes con InceptionV3\n","\n","Se utilizará InceptionV3 (que está preentrenada sobre Imagenet) para clasificar cada imagen. Se extraen las características de la útima capa convolucional.\n","\n","Primero hay que convertir las imágenes a un formato soportado por InceptionV3:\n","* Redimensionar a 299x299 pixeles\n","* Preprocesar la imagen utilizando el método $preprocess\\_input$ para normalizar la imagen de modo que cada pixel se encuentre en un rango de -1 a 1. "]},{"cell_type":"code","metadata":{"id":"-DJI_UIGiGCQ","colab_type":"code","colab":{}},"source":["# Función para preparar las imágenes para InceptionV3\n","\n","def load_image(image_path):\n","    img = tf.io.read_file(image_path)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, (299, 299))\n","    img = tf.keras.applications.inception_v3.preprocess_input(img)\n","    return img, image_path"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YCd71plwinv1","colab_type":"text"},"source":["## Inicializar InceptionV3 y cargar los pesos preentrenador de Imagenet\n","Se creará un modelo de Tensorflow Keras, la última capa de este modelo será la última capa convolucional de la arquitectura InceptionV3, esta capa tiene un tamaño de 8x8x2048.\n","\n","Se utiliza la última capa convolucional para extraer las características. \n","\n","* Se pasará cada imagen a través de la red y se guardará el vector resultante en un diccionario de la forma: (image_name : feature_vector)"]},{"cell_type":"code","metadata":{"id":"aHaDNJV8jbyy","colab_type":"code","colab":{}},"source":["image_model = tf.keras.applications.InceptionV3(include_top=False,\n","                                                weights='imagenet')\n","new_input = image_model.input\n","hidden_layer = image_model.layers[-1].output\n","\n","image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n","\n","image_features_extract_model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kE8-lFdEj7f5","colab_type":"text"},"source":["# Guardando en cach las características extraídas de InceptionV3\n","\n","Se preprocesará cada imagen con InceptionV3 y se guardarán los resultados como cache en disco. Es posible guardar los resultados en RAM y sería más rápido, pero debido a las limitaciones en cantidad de RAM, se debe realizar el cache en disco."]},{"cell_type":"code","metadata":{"id":"VbpTJXtnkVdX","colab_type":"code","colab":{}},"source":["# Se instalará y preparará tqdm para visualizar el progreso\n","!pip install tqdm\n","from tqdm import tqdm\n","\n","# Get unique images\n","encode_train = sorted(set(img_name_vector))\n","\n","# Feel free to change batch_size according to your system configuration\n","image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n","\n","image_dataset = image_dataset.map(\n","  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n","\n","for img, path in tqdm(image_dataset):\n","  batch_features = image_features_extract_model(img)\n","  batch_features = tf.reshape(batch_features,\n","                              (batch_features.shape[0], -1, batch_features.shape[3]))\n","\n","  for bf, p in zip(batch_features, path):\n","    path_of_feature = p.numpy().decode(\"utf-8\")\n","    np.save(path_of_feature, bf.numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-svI4IWsJ475","colab_type":"text"},"source":["## Preprocesando y tokenizando las descripciones\n"]},{"cell_type":"markdown","metadata":{"id":"GGpRrjSlKhO6","colab_type":"text"},"source":["Se tokenizan las descripciones separando por espacios, así obtendremos el vocabulario de todas las palabras únicas en los datos."]},{"cell_type":"code","metadata":{"id":"90gKWMq6KiVQ","colab_type":"code","colab":{}},"source":["# Se busca el tamaño máximo de las descripciones en el dataset\n","def calc_max_length(tensor):\n","    return max(len(t) for t in tensor)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A4WJ6SYJLLXT","colab_type":"text"},"source":["Se limitará el tamaño del vocabulario a 10000 palabras para utilizar menos memoria. Se reemplazarán las palabras faltantes con el token \"UKN\""]},{"cell_type":"code","metadata":{"id":"hCUyUcmmLYu3","colab_type":"code","colab":{}},"source":["# Se eligen las primeras 10000 words from the vocabulary\n","top_k = 10000\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n","                                                  oov_token=\"<unk>\",\n","                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n","tokenizer.fit_on_texts(train_captions)\n","train_seqs = tokenizer.texts_to_sequences(train_captions)\n","\n","tokenizer.word_index['<pad>'] = 0\n","tokenizer.index_word[0] = '<pad>'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e-f7GJ9yL5xx","colab_type":"text"},"source":["Se crea un mapeo [palabra-indice] y un mapeo [indice-palabra]"]},{"cell_type":"code","metadata":{"id":"HXu5VtkWMDQW","colab_type":"code","colab":{}},"source":["# Se generan los vectores de tokens\n","train_seqs = tokenizer.texts_to_sequences(train_captions)\n","\n","# Se rellena cada vector con el tamaño máximo de las descripciones presentes\n","cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n","\n","# Se calcula max_lenght, un valor que se utilzará para guardar los pesos de la atención\n","# Calculates the max_length, which is used to store the attention weights\n","# Esto es, la descripción más larga (en tokens)\n","max_length = calc_max_length(train_seqs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CxG3TUoOMrdJ","colab_type":"text"},"source":["## Se separan los datos en TRAIN y TEST"]},{"cell_type":"code","metadata":{"id":"DktLIeVPMvZV","colab_type":"code","colab":{}},"source":["# Se crean los sets de entrenamiento y validación en relación 80-20\n","img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n","                                                                    cap_vector,\n","                                                                    test_size=0.2,\n","                                                                    random_state=0)\n","\n","# imgs de entrenamiento - captions de entrenamiento - imgs de validacioon - captions de validación\n","len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KjVtRAmSOOFa","colab_type":"text"},"source":["## Se crea el dataset tf.data para el entrenamiento"]},{"cell_type":"markdown","metadata":{"id":"xgHNvn-uOUe0","colab_type":"text"},"source":["Las imágenes y las descripciones están listas. Ahora se procederá a crear el dataset de tipo tf.data para entrenar el modelo."]},{"cell_type":"code","metadata":{"id":"lk-Nxx66OepU","colab_type":"code","colab":{}},"source":["# Función para cargar los archivos de numpy\n","def map_func(img_name, cap):\n","  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n","  return img_tensor, cap\n","\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 1000\n","embedding_dim = 256\n","units = 512\n","vocab_size = top_k + 1\n","num_steps = len(img_name_train) // BATCH_SIZE\n","# La dimensión del vector extraído de InceptionV3 es (64, 2048)\n","# Las dimensiones anteriores se expresan en las variables siguientes\n","features_shape = 2048\n","attention_features_shape = 64\n","\n","dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n","\n","# Use map to load the numpy files in parallel\n","dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n","          map_func, [item1, item2], [tf.float32, tf.int32]),\n","          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","# Shuffle and batch\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WXFFmNtAO3q6","colab_type":"text"},"source":["## Generando el modelo"]},{"cell_type":"markdown","metadata":{"id":"Nc9L7b6YO7Mt","colab_type":"text"},"source":["El modelo se generará con un decodificador con la misma arquitectura al utilizado para traducción, disponible en el siguiente [paper](https://arxiv.org/pdf/1502.03044.pdf).\n","\n","La arquitectura está conformada por:\n","* Las características extraidas de la última capa convolucional de InceptionV3, resultante en un vector con  dimensión (8,8,2048).\n","* Se redimensionó a una dimensión de (64,2048).\n","* Este vector se parará a través de un codificador convolucional (que consiste en una capa completamente conectada).\n","* La red neuronal recurrente (GRU) atenderá las zonas de la imagen para generar la siguiente palabra según el foco de ATENCIÓN"]},{"cell_type":"code","metadata":{"id":"PALIZ-2MO599","colab_type":"code","colab":{}},"source":["# Módulo de atención\n","class BahdanauAttention(tf.keras.Model):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, features, hidden):\n","    # Características de salida del codificador convolucional\n","    # dimension == (batch_size, 64, embedding_dim)\n","\n","    # hidden dimension == (batch_size, hidden_size)\n","    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n","    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","\n","    # score dimension == (batch_size, 64, hidden_size)\n","    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n","\n","    # attention_weights dimension == (batch_size, 64, 1)\n","    # Se obtiene un 1 en el último eje porque se aplicó el score a self.V\n","    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * features\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights\n","\n","# Codificador convolucional\n","class CNN_Encoder(tf.keras.Model):\n","    # Como las características ya se extrajeron y obtuvieron con pickle\n","    # El codificador pasa esas características a través de una capa completamente conectada\n","    def __init__(self, embedding_dim):\n","        super(CNN_Encoder, self).__init__()\n","        # shape after fc == (batch_size, 64, embedding_dim)\n","        self.fc = tf.keras.layers.Dense(embedding_dim)\n","\n","    def call(self, x):\n","        x = self.fc(x)\n","        x = tf.nn.relu(x)\n","        return x\n","\n","# Decodificador recurrente\n","class RNN_Decoder(tf.keras.Model):\n","  def __init__(self, embedding_dim, units, vocab_size):\n","    super(RNN_Decoder, self).__init__()\n","    self.units = units\n","\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    self.fc1 = tf.keras.layers.Dense(self.units)\n","    self.fc2 = tf.keras.layers.Dense(vocab_size)\n","\n","    self.attention = BahdanauAttention(self.units)\n","\n","  def call(self, x, features, hidden):\n","    # Se define la atención como un modelo separado\n","    context_vector, attention_weights = self.attention(features, hidden)\n","\n","    # Dimension de x despúes de pasar por embedding == (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","\n","    # Dimension de x después de concatenar == (batch_size, 1, embedding_dim + hidden_size)\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","    # Se pasa el vector concatenado a la capa GRU\n","    output, state = self.gru(x)\n","\n","    # dimension == (batch_size, max_length, hidden_size)\n","    x = self.fc1(output)\n","\n","    # dimension de x == (batch_size * max_length, hidden_size)\n","    x = tf.reshape(x, (-1, x.shape[2]))\n","\n","    # dimension de salida == (batch_size * max_length, vocab)\n","    x = self.fc2(x)\n","\n","    return x, state, attention_weights\n","\n","  def reset_state(self, batch_size):\n","    return tf.zeros((batch_size, self.units))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AnqHT-qaSiV9","colab_type":"text"},"source":["Se genera el modelo a partir de las clases y métodos generados anteriormente."]},{"cell_type":"code","metadata":{"id":"Ef6wWhaYSmDn","colab_type":"code","colab":{}},"source":["encoder = CNN_Encoder(embedding_dim)\n","decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n","\n","optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_mean(loss_)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eyz5t21CSp5u","colab_type":"text"},"source":["## Checkpoint"]},{"cell_type":"code","metadata":{"id":"RBqHqiQ7SsCh","colab_type":"code","colab":{}},"source":["checkpoint_path = \"./checkpoints/train\"\n","ckpt = tf.train.Checkpoint(encoder=encoder,\n","                           decoder=decoder,\n","                           optimizer = optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","start_epoch = 0\n","if ckpt_manager.latest_checkpoint:\n","  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n","  # restoring the latest checkpoint in checkpoint_path\n","  ckpt.restore(ckpt_manager.latest_checkpoint)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CNLuH2PHTGRc","colab_type":"text"},"source":["## Entrenando el modelo\n","1. Se extrajeron las características guardadas en los archivos .npy y se pasaron a través del codificador.\n","2. La salida del codificador, el estado oculto inicializado en 0 y la entrada del decodificador (que es el token inicial) se pasan al decodificador.\n","3. El decodificador retorna las predicciones y estado oculto del decodificador.\n","4. El estado oculto del decodificador se pasa de nuevo al modelo para realizar las predicciones y calcular la pérdida.\n","5. Se utiliza el \"teacher forcing\" para decidir la siguiente entrada al decodificador.\n","* Teacher Forcing: Una técnica en donde la palabra objetivo se pasa como la siguiente entrada al decodificador.\n","6. El último paso es calcular los gradientes, aplicarlos en el optimizador y retropropagar los errores."]},{"cell_type":"code","metadata":{"id":"-uT4P-wlURHN","colab_type":"code","colab":{}},"source":["# Función que ejecuta un paso de entrenamiento\n","@tf.function\n","def train_step(img_tensor, target):\n","  loss = 0\n","\n","  # initializing the hidden state for each batch\n","  # because the captions are not related from image to image\n","  hidden = decoder.reset_state(batch_size=target.shape[0])\n","\n","  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n","\n","  with tf.GradientTape() as tape:\n","      features = encoder(img_tensor)\n","\n","      for i in range(1, target.shape[1]):\n","          # passing the features through the decoder\n","          predictions, hidden, _ = decoder(dec_input, features, hidden)\n","\n","          loss += loss_function(target[:, i], predictions)\n","\n","          # using teacher forcing\n","          dec_input = tf.expand_dims(target[:, i], 1)\n","\n","  total_loss = (loss / int(target.shape[1]))\n","\n","  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","  gradients = tape.gradient(loss, trainable_variables)\n","\n","  optimizer.apply_gradients(zip(gradients, trainable_variables))\n","\n","  return loss, total_loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KuHoxfXwUR83","colab_type":"text"},"source":["### Se comienza el entrenamiento:"]},{"cell_type":"code","metadata":{"id":"UmAdkvsJUExG","colab_type":"code","colab":{}},"source":["# Se añade la lista de loss_plot por separado para evitar que se resetee\n","# si se vyekve a ejecutar la celda de entrenamiento\n","loss_plot = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j3DYIaJ2UGcM","colab_type":"code","colab":{}},"source":["EPOCHS = 20\n","\n","for epoch in range(start_epoch, EPOCHS):\n","    start = time.time()\n","    total_loss = 0\n","\n","    for (batch, (img_tensor, target)) in enumerate(dataset):\n","        batch_loss, t_loss = train_step(img_tensor, target)\n","        total_loss += t_loss\n","\n","        if batch % 100 == 0:\n","            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n","              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n","    # storing the epoch end loss value to plot later\n","    loss_plot.append(total_loss / num_steps)\n","\n","    if epoch % 5 == 0:\n","      ckpt_manager.save()\n","\n","    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n","                                         total_loss/num_steps))\n","    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1UxdL8l3UhiH","colab_type":"text"},"source":["Se visualizan los resultados del entrenamiento en la siguiente celda."]},{"cell_type":"code","metadata":{"id":"Ql_SBQe5UhvI","colab_type":"code","colab":{}},"source":["plt.plot(loss_plot)\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Loss Plot')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uQBvtE_8Urjg","colab_type":"text"},"source":["## Evaluación del modelo\n","\n","* La evaluación del modelo es similar a la etapa de entrenamiento, con la excepción que aquí no se utilizará la técnica de \"teacher forcing\". La entrada del decodificador en cada paso serán sus predicciones anteriores junto con el estado oculto y la salida del codificador.\n","* Se detiene la predicción cuando el modelo pedice el token final.\n","* Se almacenan los pesos de atención en cada paso."]},{"cell_type":"code","metadata":{"id":"xyyAOJpTVWLp","colab_type":"code","colab":{}},"source":["# Función que evalúa la predicción de una descripción para una imagen.\n","def evaluate(image):\n","    attention_plot = np.zeros((max_length, attention_features_shape))\n","\n","    hidden = decoder.reset_state(batch_size=1)\n","\n","    temp_input = tf.expand_dims(load_image(image)[0], 0)\n","    img_tensor_val = image_features_extract_model(temp_input)\n","    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n","\n","    features = encoder(img_tensor_val)\n","\n","    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n","    result = []\n","\n","    for i in range(max_length):\n","        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n","\n","        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n","\n","        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n","        result.append(tokenizer.index_word[predicted_id])\n","\n","        if tokenizer.index_word[predicted_id] == '<end>':\n","            return result, attention_plot\n","\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","\n","    attention_plot = attention_plot[:len(result), :]\n","    return result, attention_plot\n","\n","# Función que imprime el plot de atención que se puso a una determinada imagen.\n","def plot_attention(image, result, attention_plot):\n","    temp_image = np.array(Image.open(image))\n","\n","    fig = plt.figure(figsize=(10, 10))\n","\n","    len_result = len(result)\n","    for l in range(len_result):\n","        temp_att = np.resize(attention_plot[l], (8, 8))\n","        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n","        ax.set_title(result[l])\n","        img = ax.imshow(temp_image)\n","        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n","\n","    plt.tight_layout()\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mbaXu-YaXoRO","colab_type":"code","colab":{}},"source":["# Se evalúa el modelo con una imagen aleatoria y sus predicciones\n","rid = np.random.randint(0, len(img_name_val))\n","image = img_name_val[rid]\n","\n","real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n","result, attention_plot = evaluate(image)\n","\n","print ('Real Caption:', real_caption)\n","print ('Prediction Caption:', ' '.join(result))\n","plot_attention(image, result, attention_plot)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GAQM-UOKbEWa","colab_type":"text"},"source":["## Evaluación con BLEU"]},{"cell_type":"code","metadata":{"id":"cVxiC7yKt_lS","colab_type":"code","colab":{}},"source":["Generando listas de descripciones esperadas y las generadas."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kortAfOjuFKL","colab_type":"code","colab":{}},"source":["real_captions = []\n","generated_captions = []\n","img_path = []\n","\n","n_tests = 300\n","\n","for i,image in enumerate(img_name_val[:n_tests]):\n","  real_captions.append(' '.join([tokenizer.index_word[i] for i in cap_val[i] if i not in [0]]))\n","  caption, attention_plot = evaluate(image)\n","  img_path.append(image)\n","  generated_captions.append(caption)\n","  if i % 100 == 0: print(\"Describiendo id %d\" %(i)) \n","\n","print(\"Finalizado...\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9qDNTmmIbGkq","colab_type":"code","colab":{}},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","from nltk.translate.bleu_score import sentence_bleu\n","\n","import seaborn as sns\n","\n","tokenized_real_captions = []\n","tokenized_generated_captions = []\n","# Se tokenizan las descripciones reales y se eliminan las etiquetas de\n","# <start> <end>\n","for caption in generated_captions:\n","  d_caption = caption[:len(caption)-1]\n","  tokenized_generated_captions.append(d_caption)\n","\n","for caption in real_captions:\n","  back_caption = caption\n","  back_caption = back_caption.split()\n","  back_caption = back_caption[1:-1]\n","  tokenized_real_captions.append(back_caption)\n","\n","\n","# Se separarán las buenas predicciones de las malas.\n","pred_good, pred_bad, bleus, meteors = [], [], [], []\n","for i, path in enumerate(img_path):\n","  caption_true = tokenized_real_captions[i]\n","  gen_caption = tokenized_generated_captions[i]\n","  bleu = sentence_bleu([caption_true], gen_caption)\n","  bleus.append(bleu)\n","  if bleu > 0.7:\n","    pred_good.append((bleu,path,caption_true,gen_caption))\n","  elif bleu < 0.3:\n","    pred_bad.append((bleu,path,caption_true,gen_caption))\n","\n","# Visualizando la distribución de los BLEUs\n","sns.set(color_codes = True)\n","sns.distplot(bleus)\n","\n","print(\"\\t\\t\\t BLEU\")\n","print(\"BLEU más alto: \", np.amax(bleus))\n","print(\"BLEU más bajo: \", np.amin(bleus))\n","print(\"Puntuación BLEU media {:4.3f}\".format(np.mean(bleus)))\n","print(\"Puntuación BLEU mediana {:4.3f}\".format(np.median(bleus)))\n","print(\"Desviación estándar de las puntuaciones {:4.3f}\".format(np.std(bleus)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"APff3iwaIQAY","colab_type":"text"},"source":["Se visualizan las imágenes con las predicciones buenas."]},{"cell_type":"code","metadata":{"id":"mJYS3R_aITqH","colab_type":"code","colab":{}},"source":["def plot_images(generated_captions):\n","    def create_str(caption_true):\n","        strue = \"\"\n","        for s in caption_true:\n","            strue += \" \" + s\n","        return(strue)\n","    npix = 224\n","    target_size = (npix,npix,3)    \n","    count = 1\n","    fig = plt.figure(figsize=(10,20))\n","    npic = len(generated_captions)\n","    for pb in generated_captions:\n","        bleu,jpgfnm,caption_true,caption = pb\n","        ## images \n","        filename = jpgfnm\n","        image_load = load_img(filename, target_size=target_size)\n","        ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n","        ax.imshow(image_load)\n","        count += 1\n","\n","        caption_true = create_str(caption_true)\n","        caption = create_str(caption)\n","        \n","        ax = fig.add_subplot(npic,2,count)\n","        plt.axis('off')\n","        ax.plot()\n","        ax.set_xlim(0,1)\n","        ax.set_ylim(0,1)\n","        ax.text(0,0.7,\"real:\" + caption_true,fontsize=20)\n","        ax.text(0,0.4,\"generada:\" + caption,fontsize=20)\n","        ax.text(0,0.1,\"BLEU: {}\".format(bleu),fontsize=20)\n","        count += 1\n","    plt.show()\n","\n","print(\"Descripciones malas\")\n","plot_images(pred_bad[:5])\n","print(\"Descripciones buenas\")\n","plot_images(pred_good[:5])"],"execution_count":0,"outputs":[]}]}